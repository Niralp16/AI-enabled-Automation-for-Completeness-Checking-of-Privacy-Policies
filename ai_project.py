# -*- coding: utf-8 -*-
"""AI_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1esWB-kVSyVDqV0tAQdBmU1XGwzF_voK8
"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

data = pd.read_csv("/content/gdpr_comb.csv").dropna(axis=1, how='all')   #load dataset, drop unnamed cols

# Preprocess the data
X = data.drop('Detected(V/C)', axis=1)  #  we are selecting Feature columns from our dataset
y = data['Detected(V/C)']               # Target variable

numeric_features = X.select_dtypes(include=['number']).columns     #defining the steps : preprocessing
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])          # we are preprocessing data to Handling Missing Values, Scaling numerical values ,encoding feaatures present in string as machine only understand numerical values

categorical_features = X.select_dtypes(include=['object']).columns
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])


pipeline = Pipeline(steps=[('preprocessor', preprocessor),              #preprocessing pipeline
                           ('classifier', LogisticRegression())])


pipeline.fit(X, y)       #train the lr model


chapter = input("Enter chapter number: ")                   #user provides the data for which they need the prediction
article = input("Enter article number: ")
article_title = input("Enter article title: ")
authority = input("Enter authority: ")


new_data = pd.DataFrame({                   #feed the fetures for the prediction
    'chapter': [chapter],
    'article': [article],
    'article_title': [article_title],
    'authority': [authority]

})


predictions = pipeline.predict(new_data)     #trained model predicts using the given data


print("\nPredictions:")
if predictions[0] == 1:
    print("Violation Detected")    #violation == 1 & compliance == 0.
else:
    print("No Violation Detected")

!pip install PyPDF2

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
from google.colab import files
from PyPDF2 import PdfReader
import io

# Load datasets
policies_df = pd.read_csv('/content/drive/MyDrive/gdpr_text.csv')  # Dataset containing policies/rules
violations_df = pd.read_csv('/content/drive/MyDrive/gdpr_violations.csv')  # Dataset containing violation information

# Function to preprocess text
def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    # Convert tokens to lowercase
    tokens = [token.lower() for token in tokens]
    # Remove punctuation
    tokens = [token for token in tokens if token not in string.punctuation]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    # Join tokens back into text
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text

# Compare user-provided policy with policies in the dataset
def compare_policies(user_policy_text):
    # Use TF-IDF vectorization to convert text data into numerical representations
    vectorizer = TfidfVectorizer()
    policy_vectors = vectorizer.fit_transform(policies_df['gdpr_text'])

    # Transform user policy text into numerical representation
    user_policy_vector = vectorizer.transform([user_policy_text])

    # Calculate cosine similarity between user policy and policies in the dataset
    similarities = cosine_similarity(user_policy_vector, policy_vectors)

    # Find the most similar policy in the dataset
    most_similar_index = similarities.argmax()
    most_similar_policy = policies_df.iloc[most_similar_index]['gdpr_text']

    # Optionally, check for violations based on the most similar policy
  #  violations_for_most_similar_policy = violations_df[violations_df['summary'] == most_similar_policy]
    # Check if any part of the most similar policy is present in the violation dataset
    violation_found = False
    for violation_summary in violations_df['summary']:
        if most_similar_policy in violation_summary:
            print("Violation Found:", violation_summary)
            violation_found = True

    if not violation_found:
        print("No violations found in the user-provided policy.")
    return most_similar_policy, violations_for_most_similar_policy

# Function to read PDF bytes and extract text
def read_pdf_bytes(pdf_bytes):
    reader = PdfReader(io.BytesIO(pdf_bytes))
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text

# This part of the code assumes you are using Google Colab for file upload
print("Please upload your policy file:")
uploaded = files.upload()
user_policy_bytes = list(uploaded.values())[0]
user_policy_text = read_pdf_bytes(user_policy_bytes)

# Compare user-provided policy with policies in the dataset
most_similar_policy, violations_for_most_similar_policy = compare_policies(user_policy_text)

# Print results without displaying the user-provided policy text
print('\n')
print("Most similar policy in the dataset:")
print(most_similar_policy)

# Initialize a variable to track if any violation is found
violation_found = False

# Iterate through each GDPR violation entry
for index, violation_entry in violations_df.iterrows():
    violation_summary = violation_entry['summary']
    # Check if any part of the violation summary matches with any GDPR rule
    for rule_text in policies_df['gdpr_text']:
        if rule_text in violation_summary:
            print("Violation Found:", violation_summary)
            violation_found = True
            break
    # If a violation is found, stop further iteration
    if violation_found:
        break

# If no violation is found, print appropriate message
if not violation_found:
    print("\nNo violations found in the user-provided policy.")

import matplotlib.pyplot as plt

# Function to plot similarity scores
def plot_similarity_scores(similarity_scores, policy_titles, user_policy_title):
    # Sort similarity scores and policy titles based on scores
    sorted_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)
    similarity_scores_sorted = [similarity_scores[i] for i in sorted_indices]
    policy_titles_sorted = [policy_titles[i] for i in sorted_indices]

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(similarity_scores_sorted)), similarity_scores_sorted, color='skyblue')
    plt.yticks(range(len(similarity_scores_sorted)), policy_titles_sorted)
    plt.xlabel('Similarity Score')
    plt.title(f'Top {len(similarity_scores_sorted)} Policies/Rules Similar to {user_policy_title}')
    plt.gca().invert_yaxis()  # Invert y-axis to have the highest similarity at the top
    plt.show()

# Compare user-provided policy with policies in the dataset
def compare_policies(user_policy_text):
    # Use TF-IDF vectorization to convert text data into numerical representations
    vectorizer = TfidfVectorizer()
    policy_vectors = vectorizer.fit_transform(policies_df['gdpr_text'])

    # Transform user policy text into numerical representation
    user_policy_vector = vectorizer.transform([user_policy_text])

    # Calculate cosine similarity between user policy and policies in the dataset
    similarities = cosine_similarity(user_policy_vector, policy_vectors)

    # Find the top N most similar policies in the dataset
    top_n = 5  # Define the number of top similar policies to consider
    most_similar_indices = similarities.argsort()[0][-top_n:][::-1]
    most_similar_policies = [policies_df.iloc[i]['gdpr_text'] for i in most_similar_indices]
    similarity_scores = [similarities[0][i] for i in most_similar_indices]

    # Plot similarity scores
    plot_similarity_scores(similarity_scores, most_similar_policies, "User's Policy")

    # Optionally, check for violations based on the most similar policies
    violations_for_most_similar_policies = violations_df[violations_df['summary'].isin(most_similar_policies)]

    return most_similar_policies, violations_for_most_similar_policies

# Compare user-provided policy with policies in the dataset
most_similar_policies, violations_for_most_similar_policies = compare_policies(user_policy_text)